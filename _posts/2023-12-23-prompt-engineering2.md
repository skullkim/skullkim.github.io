---
layout:       post
title:        "Chapter 2. 모든 것은 어텐션으로부터 시작되었다."
author:       "yunki kim"
header-style: text
catalog:      true
tags:
- 도서
- 프롬프트 엔지니어링
- AI
---

# 1. 챗 GPT, 바트 LaMDA, PaLM, LLaMA까지 초거대 AI의 공통점음?

- GPT, 라마 등 대부분의 LLM AI는 구글이 개발한 트랜스포머(transformer)라는 AI 기술을 개조해 만든 것이다. 그리고 트랜스포머는 어텐션(Attention)이라는 기술을 바탕으로 만들어진 AI이다. 때문에 현태의 초거대 AI는 태생적으로 어텐션의 특징을 계승한다.

# 2. 어텐션의 원리 쉽게 살펴보기

(설명이 이상해서 따로 찾아 적었습니다)

- 어텐션(Attention)은 자연어 처리와 기계번역 분야에서 사용되는 기술 중 하나로, 특정 정보에 집중하거나 중요한 부분에 가중치를 부여하는 메커니즘이다. 주어진 입력에 대해 모델이 각 입력의 중요도를 동적으로 학습하여 해당 중요도에 따라 가중치를 부여한다.
- 기존에 사용하던 순환 신경망(RNN)과 같은 시퀀스 모델링 방법은 고정된 크기의 컨텍스트 윈도우를 사용하여 입력 시퀀스를 처리한다. 이러한 방식은 긴 시퀀스에 대해 정보를 제한적으로 학습할 수 있고, 긴 거리에 있는 의존성을 캡처하기 어렵다.

# 3. 극도로 발달한 커닝 기술은 지능과 구불되지 않는다.

- LLM은 어텐션 덕분에 대량의 텍스트를 빠르게 요약하고 압축할 수 있다.

# 4. 챗 GPT는 당신이 오래전 했던 이야기를 기억한다

- LLM은 대답을 하기 전에 이전까지 나누었던 대화 대용을 참고하고 대답한다. 이 때문에 대화를 할 때 유용한 정보들을 미리 채팅창에 입력해 두어도 좋다.

출처 - [프롬프트 엔지니어링](https://product.kyobobook.co.kr/detail/S000209512470)