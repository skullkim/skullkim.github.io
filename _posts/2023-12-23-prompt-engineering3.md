---
layout:       post
title:        "Chapter 3. 당신은 LLM과 그 사용법을 오해하고 있다."
author:       "yunki kim"
header-style: text
catalog:      true
tags:
- 도서
- 프롬프트 엔지니어링
- AI
---

# 1. 할루시네이션(hallucination), AI가 문제인가? 사람이 문제인가?

- AI가 잘못된 정보를 마치 진실처럼 전달하는 현상을 할루시네이션이라 부른다.
- 많은 사람들이 할루시네이션에 대해 예민하게 반응하는데, 이는 LLM을 만물을 이해하는 박사처럼 이해하기 때문이다. 사실 LLM은 인간이 언어를 사용하는 방식에 대해 이해하고 학습한 AI이다. 지식을 정확하게 전달하기 위해 만들어진 AI가 아니다.

# 2. 인간의 말을 알아듣는 기계는 사랑받기 마련이다

- 사람의 말을 알아듣고 이해하는 AI를 연구하는 분야를 자연어 처리(NLP - Natural Language Processing)이라 한다.

# 3. 이해력을 담당하는 인코더, 표현력을 담당하는 디코더

- 손실 압축이란 하나의 정보를 더 적은 용량으로 전달하기 위해 디테일을 제거하고 저장하는 것을 의미한다. 인코딩(encoding)은 손실 압축에 해당하는 작업이며 이를 담당하는 구조물을 인코더(encoder)라고 한다. 인코딩은 외부 정보를 AI에 입력하는 과정을 의미한다. 때문에 인코더는 성능이 뛰어날수록 AI의 추상화 능력과 이해력이 높아진다.
- 반대로 압축된 정보를 끄집어내서 표현하는 과정을 디코딩(decoding)이라 한다. AI에서 디코딩을 담당하는 부분을 디코더(decoder)라고 한다. 디코더 성능이 뛰어날수록 AI의 표현력이 좋아진다. LLM의 결과는 모두 디코딩 성능에 달려있다고 볼 수 있다.

# 4. 레이턴스 스페이스(latent space), 뇌가 정보를 저장하는 원리

- 사람은 습득한 정보를 뇌에 저장한다. 뇌는 수많은 뉴런(신경 세포)이 서로 복잡하게 연결되고 얽혀서 만들어진다. 어떤 정보를 사람이 받아들였을 때 특정 뉴런이 반응한다. 특정 뉴런이 반응하는 것을 1, 반응하지 않는 것을 0으로 나타내고, 뉴런에 번호를 매길 수 있다고 해보자. 그러면 사람의 뇌가 인지한 대상에 따른 뉴런의 반응을 다음과 같이 표현할 수 있다. (0, 0, 1), (1, 0, 1), 등
- 뉴런이 n 개 있다고 해보자. 그러면 입력받은 대상에 대한 뉴런 반응을 n 차원 공간의 좌표로 표현할 수 있다(ex - (0, 1, 0, …, 1)). 즉, 벡터(vector)로 표현할 수 있다. 이 벡터라 놓이게 되는 n 차원의 가상 공간을 레이턴트 스페이스(latent space)라고 한다. 즉, 정보가 들어오면 이를 레이턴트 스페이스 위의 하나의 벡터로 정리할 수 있다. 이 벡터를 레이턴트 벡터(latent vector)라고 한다.
- 인코딩은 정보를 압출해서 레이턴트 벡터를 만드는 과정이다. 비슷한 정보들은 레이턴스 스페이스상에서 가까운 곳에 위치시키고, 상이한 정보들은 멀리 떨어진 곳에 위치시킨다. 디코더가 하는 역할은 의미를 담은 레이턴트 벡터가 가진 고유의 의미를 해석해서 사람이 이해할 수 있는 데이터로 만드는 과정이다.
- 비슷한 정보들은 비슷한 위치에 존재하는 점 때문에 새로 입력받은 정보가 레이턴스 스페이스의 어느 부분에 위치하는 벡터인지를 판단해 정보를 추론할 수도 있다.

# 5. AI가 인간의 방식으로 단어의 의미를 이해한다

- 인간은 단어를 배울 때 반복되는 힌트(데이터)와 매칭되는 단어(레이블)를 반복해서 입력받는다. 그러면 자연스레 뇌의 레이턴트 스페이스에 단어의 의미가 저장된다.
- AI도 위와 같은 방식으로 학습을 진행한다. 그러면서 문장에서 자주 등장하는 단어가 있을 때, 그 단어들의 의미가 유사하다는 점도 추론할 수 있다. 그뿐만 아니라 방대한 데이터를 학습하는 과정에서 레이턴트 스페이스 위에는 온갖 단어들이 존재하게 된다. 단어와 단어 사이의 의미 차이는 벡터 형태로 표현할 수 있기에 이를 토대로 유의어 탐색, 단어들의 관계의 유사도 역시 추론할 수 있다.

# 6. AI가 문장의 의미와 뉘양스를 이해한다

- 구글이 2014년에 공개한 seq2seq 모델은 단어를 넘어서 문장의 의미를 이해할 수 있는 고성능 AI이다. 기존 문장 의미 분석 방식은 문장에 포함된 핵심 동사나 형용사를 찾아, 이를 토대로 문장의 의미를 분석했다. 예를 들어 ‘맛있다’라는 단어가 포함된 문장은 모두 비슷하게 분류했다. 그러나 seq2seq는 그를 넘어 제대로 문장이 가진 의미를 이해할 수 있다. 예컨대, ‘A는 B가 만든 음식이 맛있다고 생각한다’와 ‘B는 A가 만든 음식이 맛있다고 생각한다’의 차이를 이해할 수 있다.
- seq2seq 모델은 단어 의미를 파악할 만한 자료를 제공하지 않은 채로 학습이 진행되었다. 그저 AI가 참고할 수 있는 인간이 작성한 문장만 주고 다짜고짜 번역을 시키면서 언어의 의미와 구조를 학습시켰다. 그럼에도 성공적으로 문장의 의미를 이해할 수 있었다. seq2seq 모델의 구조는 다음과 같다. 이런 구조를 인코더-디코더 구조라고 한다.

![seq2seq](/img/2023-12-23-prompt-engineering-3/img.png)
- 인코더는 입력받은 문장에서 문법 정보, 단어 형태같이 불필요한 정보를 모두 지우고 문장의 의미만 레이턴트 벡터로 표현한다. 디코더는 이 벡터의 의미를 해동해서 외국어로 번역된 문장을 만든다.
- seq2seq는 레이턴트 스페이스 크기가 고정되어 있다는 문제가 존재한다. 때문에 대량의 문장을 모두 레이턴트 스페이스에 저장해 사용할 수 없다. 즉, 용량이 큰 글에서는 제대로 된 성능이 나오지 않는다. 이 문제를 해결한 것이 어텐션이다.

# 7. LLM 전쟁의 원흉, 트랜스포머의 등장

- 2017년 Attention is all you need라는 제목의 논문을 통해 트랜스포머가 처음 등장했다. 트랜스포머는 여러 개의 인코더와 디코더가 사용되었고, 모든 모듈에 어텐션이 부착되었다. 트랜스포머 구조는 다음과 같다.
![transpormer](/img/2023-12-23-prompt-engineering-3/img1.png)

8. GPT와 BERT의 등장

![transpormer, bert](/img/2023-12-23-prompt-engineering-3/img2.png)
- 위 그림과 같이 버트는 오직 인코더로만, GPT는 오직 디코더로만 이루어져 있다. 즉, GPT는 표현력에, 버트는 이해력에 올인한 AI이다. 실제 성능을 보면 버트가 GPT보다 월등하다.

# 9. OpenAI의 급발진

- BERT의 논문을 보면 다음과 같은 문장이 나온다. ‘AI의 부피를 두 배 키웠더니 성능은 5%밖에 증가하지 않더라’. 여기서 부피란 인공 신경의 개수이다. 개수가 많아지면 부피가 커진다. 그에 따라 컴퓨터의 계산 부담이 증가한다.
- OpenAI가 2020년에 발표한 GPT-3는 BERT보다 583배 부피가 크지만 고작 몇 % 정도의 성능 차이만 존재한다. 이처럼 상식적인 규모를 벗어나는 거대한 AI를 초거대 AI라고 부른다.

# 10. 크게, 크게, 무조건 크게! 근데 이게 맞나?

- OpenAI는 효율을 포기하고 무작정 부피를 키우는 것이 AI의 성능 개선으로 이어진다는 사실을 증명했다.
- 구글도 초거대 AI를 연구하고 있지만 OpenAI 만큼 부피에 집작하지는 않는다.
- 메타는 OpenAI 행보에 반감을 가지는 것으로 보인다. 메타는 라마(LLaMA)를 발표하면서 GPT-3보다 절만 크기밖에 안 되는 AI에 데이터를 네 배 제공하면 성능이 더 좋아진다고 설명했다. 이를 토대로 LLM 운영 과정에서 탄소 배출량을 줄일 수 있다는 주장을 했다.
- 그럼에도 현제 널리 알려진 모든 LLM들은 어텐션을 적극적으로 사용하고 있다는 공통점이 있다.

# 11. 지식을 주입하는 단계는 존재하지 않는다

- GPT는 트랜스포머라는 AI를 언어와는 관련 있지만 채팅과는 무관한 다른 임무에 투입해 학습시킨다. 그리고 학습이 되면 이를 채팅을 위한 도구로 활용한다. 다른 LLM도 대체로 비슷한 방식을 사용한다.
- GPT는 학습 과정에서 문장의 일부를 트랜스포머에게 보여주고, 문장을 완성하게 한다. 이 과정에서 어텐션이 작동한다. 이 학습을 통해 AI는 문법적 구조, 논리의 전개 순서를 학습해서 자연스레 문법과 논리 구조를 이해하게 된다. 이때, 어텐션은 핵심이 되는 정보에 집중할 수 있게 해준다. 물론 어텐션이 더 정교하게 올바른 정보에 집중할 수 있게 어텐션 자체가 학습되기도 한다. 이 과정에서 AI가 지식을 따로 암기하거나 공부하는 과정은 포함되지 않는다. 그저 많고 다양한 정보가 희석되어 레이턴스 스페이스 위에 남기에 질문에 그럴싸한 답변을 생성한다.
- LLM의 본질은 인간의 언어를 잘 이해한 AI 이자 그럴싸한 답변을 잘 만들어내는 기계이다. 절대로 만물을 이해하지 않는다. 또 한, 어텐션이라는 기술에 지나치게 의존하다 보니 여러 한계를 갖는다. 프롬프트 엔지니어링은 이와 같은 LLM의 본질을 이해하고, 어텐션에 휘둘리는 AI의 특징을 집요하게 활용하여 보다 나은 결과를 만들어내는 AI 활용 방법이다.

출처 - [프롬프트 엔지니어링](https://product.kyobobook.co.kr/detail/S000209512470)